= HDFS Connector - Mule 4
ifndef::env-site,env-github[]
include::_attributes.adoc[]
endif::[]

Support Category: https://www.mulesoft.com/legal/versioning-back-support-policy#anypoint-connectors[Select]

HDFS Connector v6.0

Anypoint Connector for the Hadoop Distributed File System (HDFS) (HDFS Connector) is used as a bidirectional gateway between Mule apps and HDFS.

Release Notes: xref:release-notes::connector/hdfs-connector-release-notes-mule-4.adoc[HDFS Connector Release Notes]

== Prerequisites

This document assumes that you are familiar with Mule and HDFS.

To use the HDFS connector, you need:

* Anypoint Studio version 7.0 or later
* An instance of Hadoop Distributed File System up and running.

== Compatibility

HDFS Hadoop connector is compatible with the following:

[%header,width="100%",cols="50%,50%"]
|===
|Application/Service|Version
|Mule  |4.0 or newer
|Apache Hadoop |2.8.1 or newer
|===

== Add the Connector to a Studio Project

Anypoint Studio provides two ways to add the connector to your Studio project: from the Exchange button in the Studio taskbar or from the Mule Palette view.

=== Add the Connector Using Exchange

. In Studio, create a Mule project.
. Click the Exchange icon *(X)* in the upper-left of the Studio task bar.
. In Exchange, click *Login* and supply your Anypoint Platform username and password.
. In Exchange, search for "hdfs".
. Select the connector and click *Add to project*.
. Follow the prompts to install the connector.

=== Add the Connector in Studio

. In Studio, create a Mule project.
. In the Mule Palette view, click *(X) Search in Exchange*.
. In *Add Modules to Project*, type "hdfs" in the search field.
. Click this connector's name in *Available modules*.
. Click *Add*.
. Click *Finish*.

== Configure the Connector Global Element

To use the HDFS connector in your Mule application, you must configure a global HDFS element that can be used by the connector. The HDFS connector offers the following global configuration options, requiring the following credentials.

=== Simple Authentication Configuration

[%header,cols="50a,50a"]
|===
|Field |Description
|NameNode URI |The URI of the file system to connect to.

This is passed to the HDFS client as the FileSystem#FS_DEFAULT_NAME_KEY configuration entry. It can be overridden by values in configurationResources and configurationEntries.
|Username | User identity that Hadoop uses for permissions in HDFS.

When Simple Authentication is used, Hadoop requires the user to be set as a System Property called HADOOP_USER_NAME. If you fill this field then the connector sets it for you, however you can set it by yourself. If the variable is not set, Hadoop uses the current logged in OS user.
|Configuration Resources |A list of configuration resource files to be loaded by the HDFS client. Here you can provide additional configuration files. For example, core-site.xml.
|Configuration Entries |A map of configuration entries to be used by the HDFS client. Here you can provide additional configuration entries as key/value pairs.
|===

image::hdfs/hdfs-config.png[]


=== Kerberos Authentication Configuration

[%header,cols="50a,50a"]
|===
|Field |Description
|NameNode URI |The URI of the file system to connect to.

This is passed to HDFS client as the FileSystem#FS_DEFAULT_NAME_KEY configuration entry. It can be overridden by values in configurationResources and configurationEntries.
|Username | Kerberos principal.

This is passed to HDFS client as the "hadoop.job.ugi" configuration entry. It can be overridden by values in configurationResources and configurationEntries. If not provided, it uses the currently logged in user.
|KeytabPath |Path to the keytab file associated with username.

KeytabPath is used in order to obtain TGT from "Authorization server".  If not provided it looks for a TGT associated to username within your local kerberos cache.
|Configuration Resources |A list of configuration resource files to be loaded by the HDFS client. Here you can provide additional configuration files. For example, core-site.xml.
|Configuration Entries |A map of configuration entries to be used by the HDFS client. Here you can provide additional configuration entries as key/value pairs.
|===

image::hdfs/hdfs-config-with-kerberos.png[]

== Using the Connector

You can use this connector as an inbound endpoint for polling content of a file at a configurable rate (interval) or as an outbound connector for manipulating data into the HDFS server.

=== Connector Namespace and Schema

When designing your application in Studio, the act of dragging the connector from the palette onto the Anypoint Studio canvas should automatically populate the XML code with the connector namespace and schema location.

Namespace: `+http://www.mulesoft.org/schema/mule/hdfs+` +
Schema Location: `+http://www.mulesoft.org/schema/mule/connector/current/mule-hdfs.xsd+`


If you are manually coding the Mule app in Studio's XML editor or another text editor, define the namespace and schema location in the header of your Configuration XML, inside the `<mule>` tag.

[source,xml,linenums]
----
 <mule xmlns:http="xmlns:hdfs="http://www.mulesoft.org/schema/mule/hdfs"
      	xmlns:ee="http://www.mulesoft.org/schema/mule/ee/core"
      	xmlns="http://www.mulesoft.org/schema/mule/core"
       xmlns:doc="http://www.mulesoft.org/schema/mule/documentation"
      	xmlns:spring="http://www.springframework.org/schema/beans"
      	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
      	xsi:schemaLocation="

      http://www.mulesoft.org/schema/mule/http http://www.mulesoft.org/schema/mule/http/current/mule-http.xsd
      http://www.mulesoft.org/schema/mule/ee/core http://www.mulesoft.org/schema/mule/ee/core/current/mule-ee.xsd
      http://www.mulesoft.org/schema/mule/core http://www.mulesoft.org/schema/mule/core/current/mule.xsd
      http://www.mulesoft.org/schema/mule/hdfs http://www.mulesoft.org/schema/mule/hdfs/current/mule-hdfs.xsd">
</mule>
----

Also ensure your POM file has this dependency:

[source,xml,linenums]
----
<dependency>
  <groupId>org.mule.connectors</groupId>
  <artifactId>mule-hdfs-connector</artifactId>
  <version>RELEASE</version>
  <classifier>mule-plugin</classifier>
</dependency>
----

Mule converts RELEASE to the latest version.

=== Example Use Case

The following example shows how to create a text file into HDFS using the connector:

. In Anypoint Studio, click File > New > Mule Project, name the project, and click OK.
. In the search field, type "http" and drag the HTTP connector to the canvas, click the green plus sign to the right of Connector Configuration, and in the next screen, click OK to accept the default settings. Name the endpoint /createFile.
. In the Search bar type hdfs and drag the HDFS connector onto the canvas.
. Choose Write to path as an operation. Set Path to `/test.txt` (this is the path of the file that is going to be created into HDFS) and leave other options with default values.
. Run the application. From your favorite HTTP client, make a POST request with "Content-type:plain/text" to `+locahost:8081/createFile+` with content that you want to write as payload. (for example, `curl -X POST -H "Content-Type:plain/text" -d "payload to write to file" localhost:8090/createFile`)
. Check that /test.txt has been created and has your content by using Hadoop explorer.


== Resources

* http://hadoop.apache.org/releases.html[Hadoop Distributed File System download]
* https://web.mit.edu/kerberos/krb5-1.12/doc/basic/keytab_def.html[keytab file]
* https://help.mulesoft.com[MuleSoft Help Center]